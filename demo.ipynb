{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictac import TicTacToeEnv\n",
    "from opponents import Opponent, RuleBasedOpponent\n",
    "from model import DQN, minimaxDQN, PolicyNet\n",
    "from dataset import ReplayDataset\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins\tlosses\tdraws\n",
      "100\t0\t0\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToeEnv(max_turns=30)\n",
    "model = minimaxDQN()\n",
    "policynet = PolicyNet()\n",
    "\n",
    "policynet.load_state_dict(torch.load('saved_models/policynet.pth', weights_only=True))\n",
    "model.load_state_dict(torch.load('saved_models/minmax_dqn3.pth', weights_only=True))\n",
    "\n",
    "opponents = [Opponent(), RuleBasedOpponent()]\n",
    "# opponent = Opponent()\n",
    "\n",
    "EPISODES = 100\n",
    "LOOK_AHEAD = 1\n",
    "wins, losses, draws = 0, 0, 0\n",
    "for episode in range(EPISODES):\n",
    "\n",
    "    opponent_num = 0\n",
    "    opponent = opponents[opponent_num]\n",
    "\n",
    "    observation, info = env.reset()\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        \n",
    "        opponent_action = opponent.choose_action(env)\n",
    "        # opponent_action = model.choose_action(-observation['board'], best=False)\n",
    "\n",
    "        player_action = model.choose_action(observation['board'], opponent_num, policynet, depth=LOOK_AHEAD, best=True)\n",
    "        # player_action = model.choose_action(observation['board'], opponent_action, best=True)\n",
    "\n",
    "        # clear_output(wait=False)\n",
    "        observation, reward, terminated, _, info = env.step(player_action, opponent_action)\n",
    "        # print(env.render())\n",
    "\n",
    "    if info['winner'] == -1: losses += 1\n",
    "    elif info['winner'] == 1:\n",
    "        wins += 1\n",
    "        # print(player_action, opponent_action)\n",
    "        # print(env.render())\n",
    "    elif info['winner'] is None: draws += 1\n",
    "\n",
    "    clear_output(wait=False)\n",
    "    print(\"wins\", \"losses\", \"draws\", sep=\"\\t\")\n",
    "    print(wins, losses, draws, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(torch.tensor([-7.8296, -7.5128, -6.5433, -7.7056, -5.6361, -7.7585, -7.3463, -8.6944,\n",
    "        -7.3719]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "state = obs['board']\n",
    "\n",
    "state[[0, 4]] = -1\n",
    "state[[2, 8]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}-1.0 & 0.0 & 1.0\\\\0.0 & -1.0 & 0.0\\\\0.0 & 0.0 & 1.0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-1.0,  0.0, 1.0],\n",
       "[ 0.0, -1.0, 0.0],\n",
       "[ 0.0,  0.0, 1.0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix(state.reshape((3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQN()\n",
    "dqn.load_state_dict(torch.load(\"saved_models/dqn2.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}-5.72 & -4.41 & -7.92 & -4.5 & -7.23 & -7.42 & -4.88 & -5.93 & -6.29\\\\-1.35 & -3.18 & -1.92 & -0.18 & -0.39 & -3.0 & -1.06 & 0.34 & -1.93\\\\-7.2 & -6.09 & -7.21 & -7.05 & -7.25 & -6.49 & -6.11 & -7.3 & -6.34\\\\-2.21 & -0.7 & -2.16 & -2.48 & -1.7 & -1.43 & 1.51 & -0.79 & -1.49\\\\-7.48 & -2.6 & -7.59 & -5.71 & -5.88 & -3.18 & -5.32 & -6.28 & -7.37\\\\6.33 & 8.32 & -1.8 & 6.67 & 5.29 & -2.05 & 6.62 & 7.49 & -0.33\\\\-2.19 & 1.2 & -1.39 & -1.21 & -1.57 & -1.59 & -2.07 & 0.54 & 0.14\\\\-2.98 & 0.17 & -4.22 & -0.52 & -1.83 & -4.76 & -1.68 & -2.61 & -3.15\\\\-8.36 & -6.33 & -6.48 & -6.5 & -7.16 & -6.77 & -6.17 & -7.46 & -6.23\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-5.72, -4.41, -7.92,  -4.5, -7.23, -7.42, -4.88, -5.93, -6.29],\n",
       "[-1.35, -3.18, -1.92, -0.18, -0.39,  -3.0, -1.06,  0.34, -1.93],\n",
       "[ -7.2, -6.09, -7.21, -7.05, -7.25, -6.49, -6.11,  -7.3, -6.34],\n",
       "[-2.21,  -0.7, -2.16, -2.48,  -1.7, -1.43,  1.51, -0.79, -1.49],\n",
       "[-7.48,  -2.6, -7.59, -5.71, -5.88, -3.18, -5.32, -6.28, -7.37],\n",
       "[ 6.33,  8.32,  -1.8,  6.67,  5.29, -2.05,  6.62,  7.49, -0.33],\n",
       "[-2.19,   1.2, -1.39, -1.21, -1.57, -1.59, -2.07,  0.54,  0.14],\n",
       "[-2.98,  0.17, -4.22, -0.52, -1.83, -4.76, -1.68, -2.61, -3.15],\n",
       "[-8.36, -6.33, -6.48,  -6.5, -7.16, -6.77, -6.17, -7.46, -6.23]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vals = np.round(model.forward(torch.tensor(state).to(torch.float)).detach().reshape((9, 9)), 2)\n",
    "\n",
    "Matrix(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(state).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  0.,  1.],\n",
       "       [ 0., -1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.reshape((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policynet.return_policy(state, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0.03 & 0 & 0.12 & 0 & 0.28 & 0.42 & 0.14 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[0, 0.03, 0, 0.12, 0, 0.28, 0.42, 0.14, 0]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix(policynet.return_policy(state, 0).numpy().round(2).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeEnv(max_turns=30)\n",
    "model = minimaxDQN()\n",
    "model.load_state_dict(torch.load('saved_models/minmax_dqn3.pth', weights_only=True))\n",
    "\n",
    "opponent = RuleBasedOpponent()\n",
    "opponent = Opponent()\n",
    "\n",
    "EPISODES = 100\n",
    "\n",
    "wins, losses, draws = 0, 0, 0\n",
    "for episode in range(EPISODES):\n",
    "    observation, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        \n",
    "        \n",
    "        opponent_action = opponent.choose_action(env)\n",
    "        # opponent_action = model.choose_action(-observation['board'], best=False)\n",
    "\n",
    "        player_action = model.choose_action(observation['board'], best=True)\n",
    "        # player_action = model.choose_action(observation['board'], opponent_action, best=True)\n",
    "\n",
    "        # clear_output(wait=False)\n",
    "        observation, reward, terminated, _, info = env.step(player_action, opponent_action)\n",
    "        # print(env.render())\n",
    "\n",
    "    if info['winner'] == -1: losses += 1\n",
    "    elif info['winner'] == 1:\n",
    "        wins += 1\n",
    "        # print(player_action, opponent_action)\n",
    "        # print(env.render())\n",
    "    elif info['winner'] is None: draws += 1\n",
    "\n",
    "print(\"wins\", \"losses\", \"draws\", sep=\"\\t\")\n",
    "print(wins, losses, draws, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "{'board': array([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  1.,  0.])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated:\n\u001b[1;32m----> 8\u001b[0m     player_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     opponent_action \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mchoose_action(\u001b[38;5;241m-\u001b[39mobservation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboard\u001b[39m\u001b[38;5;124m'\u001b[39m], best\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m     clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "model = minimaxDQN()\n",
    "model.load_state_dict(torch.load('saved_models/minmax_dqn3.pth', weights_only=True))\n",
    "\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    player_action = int(input())\n",
    "    opponent_action = model.choose_action(-observation['board'], best=True)\n",
    "\n",
    "    clear_output(wait=False)\n",
    "    observation, reward, terminated, _, info = env.step(player_action, opponent_action)\n",
    "\n",
    "    print(env.render())\n",
    "    print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
